contains the implementation of various optimizer builders for training neural networks in the project. These builders are responsible for constructing the optimizer graph, initializing tensors, and performing weight updates based on computed gradients during training. The directory includes implementations for Adam, Lamb, and SGD optimizers, each with their own builder classes. The optimizers support distributed training and mixed-precision weight updates.