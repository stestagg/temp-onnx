contains the implementation of various operations and components for the BERT model in the ONNX Runtime project. These operations include Attention, FastGelu, GemmFastGelu, LayerNorm, MultiHeadAttention, SkipLayerNorm, and TransformerOptions. The code is optimized for the ROCm execution provider, which is specific to Radeon Open Compute (ROCm) hardware. The implementation includes functions for computing attention scores, applying softmax, performing matrix multiplications, and handling past states. It also includes support for key padding masks, caching of key-value pairs, and scaling/biasing of input data. The directory also contains subdirectories for batched matrix multiplication, softmax, and permutation operations, with specialized classes and functions for different data types and configurations. Overall, the "bert" directory plays a crucial role in providing the functionality required for the BERT model in the ONNX Runtime project on ROCm hardware.