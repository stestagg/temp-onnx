contains CUDA implementations of various operations for quantization and ordered attention in the ONNX Runtime project. It includes files for implementing the QOrderedAttention operator used in the BERT model, as well as functions for quantizing and dequantizing floating-point values, performing layer normalization, and matrix multiplication. These CUDA kernels are designed to optimize computation on CUDA-enabled devices and enable efficient and optimized computation for attention calculations in the project.