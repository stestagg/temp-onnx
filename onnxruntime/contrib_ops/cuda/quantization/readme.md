contains CUDA implementations for quantization and dequantization operations in the ONNX Runtime project. It includes files for performing quantized attention computation, dequantizing linear operations with bias, and converting floating-point tensors to integer tensors and vice versa using linear scaling. These CUDA kernels are designed to optimize computation on CUDA-enabled devices and enable efficient and optimized computation for attention calculations in the project. Additionally, the "qordered_ops" subdirectory contains CUDA implementations for various operations related to quantization and ordered attention, including the QOrderedAttention operator used in the BERT model, layer normalization, and matrix multiplication.