contains code for running inference on a machine learning model using the ONNX Runtime C++ API. It includes functions for creating sessions, running sessions with input data, and verifying the output results. The code also provides options for using different execution providers, such as CUDA and DNNL, to optimize performance. The tests in this directory demonstrate the functionality of running inference on a model using global thread pools with different providers.