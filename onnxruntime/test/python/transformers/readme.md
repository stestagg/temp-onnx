contains code and test data related to the implementation and testing of various attention mechanisms used in the project. It includes code for creating and saving ONNX models for different types of attention layers, such as BERT attention, 3D attention, GPT-2 attention, and T5 attention. The code defines the structure of the attention subgraphs, initializes tensors and nodes, and provides functions for creating the attention layer, embedding layer, and fused versions of the layers. The directory also includes utility functions for loading and manipulating ONNX models with external data, optimizing and running inference on PyTorch models using ONNXRuntime, and performing parity tests to ensure the correctness of the models. Additionally, the directory contains unit tests for testing the fusion functionality, benchmarking the GPT-2 model, converting models to ONNX format, optimizing and validating the performance of various models, and testing the parity between different model implementations. Overall, the "transformers" directory plays a crucial role in the project by providing the necessary code and tests for attention mechanisms used in natural language processing tasks.